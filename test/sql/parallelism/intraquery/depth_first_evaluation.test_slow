# name: test/sql/parallelism/intraquery/depth_first_evaluation.test_slow
# description: Test that query plans are evaluated in a depth-first fashion
# group: [intraquery]

# we need a persistent DB because we want to compress the table that we're working with
load __TEST_DIR__/depth_first_evaluation.db

# we don't want any disk spilling because we're testing memory pressure
statement ok
SET temp_directory = ''

# 2GiB is pretty tight, each single aggregation should take only slightly less
# in this test, we're testing that we don't have multiple aggregations active simultaneously
# so this limit should be tight enough
statement ok
SET memory_limit = '2GiB'

statement ok
SET threads = 4

# we don't want this to mess up our tests
statement ok
SET disabled_optimizers TO 'common_subplan'

# 10M integers but the table is tiny because of delta compression
statement ok
CREATE TABLE integers AS SELECT range i FROM range(10_000_000)

# one of these should easily fit in memory
query I
SELECT count(*) c FROM (SELECT DISTINCT i FROM integers)
----
10000000

# the next query performs 10 of the same distinct aggregations and unions them together

# the idea here is that if DuckDB would do breadth-first plan evaluation (like it did before)
# DuckDB would first perform the 'Sink' for every distinct aggregation one by one
# this would create a HUGE temporary intermediates
# only after that DuckDB would perform the 'Finalize' for every distinct aggregation one by one
# the 'Finalize' reduces the data size to a single row
# so, this used to throw an OOM exception given the current memory limit

# with depth-first plan evaluation, DuckDB performs 'Finalize' for every distinct aggregation,
# before starting 'Sink' on the next distinct aggregation
# now this query completes without much memory pressure!
query I
SELECT sum(c)
FROM (
    SELECT count(DISTINCT i) c FROM integers
    UNION ALL
    SELECT count(DISTINCT i) c FROM integers
    UNION ALL
    SELECT count(DISTINCT i) c FROM integers
    UNION ALL
    SELECT count(DISTINCT i) c FROM integers
    UNION ALL
    SELECT count(DISTINCT i) c FROM integers
    UNION ALL
    SELECT count(DISTINCT i) c FROM integers
    UNION ALL
    SELECT count(DISTINCT i) c FROM integers
    UNION ALL
    SELECT count(DISTINCT i) c FROM integers
    UNION ALL
    SELECT count(DISTINCT i) c FROM integers
    UNION ALL
    SELECT count(DISTINCT i) c FROM integers
)
----
100000000

# same but for sorting
query I
SELECT max(i) FROM (FROM integers ORDER BY i OFFSET 9_999_999);
----
9999999

query I
SELECT max(i)
FROM (
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_990)
    UNION ALL
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_991)
    UNION ALL
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_992)
    UNION ALL
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_993)
    UNION ALL
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_994)
    UNION ALL
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_995)
    UNION ALL
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_996)
    UNION ALL
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_997)
    UNION ALL
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_998)
    UNION ALL
    SELECT max(i) AS i FROM (FROM integers ORDER BY i OFFSET 9_999_999)
)
----
9999999

statement ok
DROP TABLE integers

# column i has 0, 100, 200, etc., around 100 unique values spread out across the range 0 to 10 million
# all other values in column j are equal to range + 0.5
# column j and k are just ranges from 0 to 10 million
# we have to do this so our statistics propagation and dynamic join filters don't trivialise the query
statement ok
set disabled_optimizers to 'statistics_propagation,join_filter_pushdown,common_subplan'

statement ok
CREATE TABLE doubles (
    i DOUBLE,
    j DOUBLE,
    k0 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
    k1 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
    k2 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
    k3 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
    k4 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
    k5 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
    k6 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
    k7 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
    k8 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
    k9 DOUBLE GENERATED ALWAYS AS (j) VIRTUAL,
)

statement ok
INSERT INTO doubles
SELECT
    CAST(CASE WHEN range % 100_000 = 0 THEN range ELSE range + 0.5 END AS DOUBLE) i,
    range j,
FROM range(10_000_000)

# one of these should always fit in memory
# the idea is that the cte is a large join (10m x 10m)
# but it's really selective, only 100 tuples come out of it

# then, we join with doubles union'ed with itself, so that it becomes the probe pipeline,
# i.e., it has a higher cardinality than the selective join, which goes into a build
query I
WITH c AS NOT MATERIALIZED (
    SELECT d0.k0
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
)
SELECT count(*)
FROM (
    SELECT * FROM doubles
    UNION ALL
    SELECT * FROM doubles
) d
JOIN c
ON (d.k0 = c.k0)
----
200

# now we just crank up the number of ctes that we're joining with to 10

# again, if DuckDB would do breadth-first plan evaluation (like it did before)
# DuckDB would 'Sink' into all of of the builds in the cte's one by one, creating huge intermediates
# only after that it would perform all the selective joins and reduce the size of the intermediates
# so, this used to throw an OOM exception

# with depth-first plan evaluation, DuckDB performs the selective joins one by one,
# reducing the size of intermediates immediately, and the query completes!
query I
WITH c0 AS NOT MATERIALIZED (
    SELECT d0.k0
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
), c1 AS NOT MATERIALIZED (
    SELECT d0.k1
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
), c2 AS NOT MATERIALIZED (
    SELECT d0.k2
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
), c3 AS NOT MATERIALIZED (
    SELECT d0.k3
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
), c4 AS NOT MATERIALIZED (
    SELECT d0.k4
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
), c5 AS NOT MATERIALIZED (
    SELECT d0.k5
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
), c6 AS NOT MATERIALIZED (
    SELECT d0.k6
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
), c7 AS NOT MATERIALIZED (
    SELECT d0.k7
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
), c8 AS NOT MATERIALIZED (
    SELECT d0.k8
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
), c9 AS NOT MATERIALIZED (
    SELECT d0.k9
    FROM doubles d0
    JOIN doubles d1
    ON (d0.i = d1.j)
)
SELECT count(*)
FROM (
    SELECT * FROM doubles
    UNION ALL
    SELECT * FROM doubles
) d
JOIN c0
ON (d.k0 = c0.k0)
JOIN c1
ON (d.k1 = c1.k1)
JOIN c2
ON (d.k2 = c2.k2)
JOIN c3
ON (d.k3 = c3.k3)
JOIN c4
ON (d.k4 = c4.k4)
JOIN c5
ON (d.k5 = c5.k5)
JOIN c6
ON (d.k6 = c6.k6)
JOIN c7
ON (d.k7 = c7.k7)
JOIN c8
ON (d.k8 = c8.k8)
JOIN c9
ON (d.k9 = c9.k9)
----
200
